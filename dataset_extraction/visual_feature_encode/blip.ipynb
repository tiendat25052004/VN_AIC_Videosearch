{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":2098.358862,"end_time":"2023-10-02T02:40:47.345310","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-02T02:05:48.986448","version":"2.4.0"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9427066,"sourceType":"datasetVersion","datasetId":5726674},{"sourceId":9427072,"sourceType":"datasetVersion","datasetId":5726679}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install salesforce-lavis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import module\nimport os\nimport glob\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom lavis.models import load_model_and_preprocess","metadata":{"papermill":{"duration":3.537078,"end_time":"2023-10-02T02:06:14.355483","exception":false,"start_time":"2023-10-02T02:06:10.818405","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-21T17:50:53.775554Z","iopub.execute_input":"2024-09-21T17:50:53.775903Z","iopub.status.idle":"2024-09-21T17:50:53.781205Z","shell.execute_reply.started":"2024-09-21T17:50:53.775868Z","shell.execute_reply":"2024-09-21T17:50:53.780173Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T19:07:15.285089Z","iopub.execute_input":"2024-09-21T19:07:15.285742Z","iopub.status.idle":"2024-09-21T19:07:22.055130Z","shell.execute_reply.started":"2024-09-21T19:07:15.285699Z","shell.execute_reply":"2024-09-21T19:07:22.054239Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Parse data path","metadata":{"papermill":{"duration":0.002507,"end_time":"2023-10-02T02:06:14.360741","exception":false,"start_time":"2023-10-02T02:06:14.358234","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keyframes_dir_list = [f'/kaggle/input/{x}/Keyframes' for x in os.listdir('/kaggle/input')]\nall_keyframe_paths = dict()\nfor keyframe_dir in keyframes_dir_list:\n    for part in sorted(os.listdir(keyframe_dir)):\n        data_part = part.split('_')[-2] # L01, L02 for ex\n        all_keyframe_paths[data_part] =  dict()\nfor keyframe_dir in keyframes_dir_list:\n    for data_part in sorted(all_keyframe_paths.keys()):\n        data_part_path = f'{keyframe_dir}/{data_part}_extra'\n        if os.path.isdir(data_part_path):\n            video_dirs = sorted(os.listdir(data_part_path))\n            video_ids = [video_dir.split('_')[-1] for video_dir in video_dirs]\n            for video_id, video_dir in zip(video_ids, video_dirs):\n                keyframe_paths = sorted(glob.glob(f'{data_part_path}/{video_dir}/*.jpg'))\n                all_keyframe_paths[data_part][video_id] = keyframe_paths","metadata":{"papermill":{"duration":0.013787,"end_time":"2023-10-02T02:06:14.377050","exception":false,"start_time":"2023-10-02T02:06:14.363263","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-21T17:39:41.583510Z","iopub.execute_input":"2024-09-21T17:39:41.584418Z","iopub.status.idle":"2024-09-21T17:40:33.098118Z","shell.execute_reply.started":"2024-09-21T17:39:41.584365Z","shell.execute_reply":"2024-09-21T17:40:33.097278Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.002442,"end_time":"2023-10-02T02:06:23.433706","exception":false,"start_time":"2023-10-02T02:06:23.431264","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bs = 256\nsave_dir = './BLIP_features'\nif not os.path.exists(save_dir):\n    os.mkdir(save_dir)\n\nfor key, video_keyframe_paths in tqdm(all_keyframe_paths.items()):\n    video_ids = sorted(video_keyframe_paths.keys())\n    \n    if not os.path.exists(os.path.join(save_dir, key)):\n        os.mkdir(os.path.join(save_dir, key))\n    \n    for video_id in tqdm(video_ids):\n        video_feats = []\n        video_keyframe_path = video_keyframe_paths[video_id]\n        for i in range(0, len(video_keyframe_path), bs):\n            # Support batchsize inferencing\n            images = []\n            image_paths = video_keyframe_path[i:i+bs]\n            for image_path in image_paths:\n                image = vis_processors[\"eval\"](Image.open(image_path).convert(\"RGB\")).unsqueeze(0)\n                images.append(image)\n            images = torch.cat(images).to(device)\n            sample = {\"image\": images}\n            with torch.no_grad():\n                image_feats = model.extract_features(sample, mode=\"image\").image_embeds_proj[:,0,:]\n\n            for b in range(image_feats.shape[0]):\n                video_feats.append(image_feats[b].detach().cpu().numpy().astype(np.float32).flatten())\n        \n        np.save(f'{save_dir}/{key}/{video_id}.npy', video_feats)","metadata":{"papermill":{"duration":2051.513776,"end_time":"2023-10-02T02:40:44.729565","exception":false,"start_time":"2023-10-02T02:06:33.215789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-03T06:12:15.894944Z","iopub.execute_input":"2024-09-03T06:12:15.895712Z"},"trusted":true},"execution_count":null,"outputs":[]}]}